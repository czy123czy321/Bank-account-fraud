{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "622225bc-b011-40d1-a79f-6242f59be99d",
   "metadata": {},
   "source": [
    "feature engineering to prepare training data for XGBoost and logistic regression.\n",
    "1. Build a switcher where I can turn on and off each feature engineering step. In the notebook. should I still build the feature engineering steps on top of each other? i.e. ohe on imputed df.\n",
    "2. Outliers: We don't want to remove it, leaving them untouched for XGBoost. However, logistic regression is more sensitive to outliers, so we need to handle it.\n",
    "3. Missing values: XGBoost is good at dealing missing values. However, missing values are still imputed to prepare for other algorithm and compare model performance. \n",
    "4. Scaling: scale the data with robust scaler because there are significant outliers, still scaling is not very helpful for xgboost but we add it case we are trying other algorithms.\n",
    "5. Mutual information evaluation after all the preprocessing to find significant input features. \n",
    "6. Build a base model evalution for the dataset, which is evaluated by recall@5% because it's highly imbalanced. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60767c46-e67b-4dc2-8a6d-9845f4989725",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc092e49-eb61-4a02-97f9-ee85b66858e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"feature_engineering\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "655d2e2e-31a6-433e-9b95-4ac2a633fce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_location': '/data',\n",
       " 'output_location': '/output',\n",
       " 'code_location': '/code',\n",
       " 'config_location': '/config'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "with open('../params.yaml', 'r') as file:\n",
    "    params = yaml.safe_load(file)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cac7f9a4-40a5-4e85-9c7e-bcd0f8320382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is stored at /Users/zoe/Documents/Bank-account-fraud/data\n"
     ]
    }
   ],
   "source": [
    "dir_path = os.getcwd()\n",
    "parent_dir = os.path.dirname(dir_path)\n",
    "data_folder = parent_dir+params['data_location']\n",
    "\n",
    "print('Data is stored at', data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ea7c6d9-d28f-4080-99df-24325cc6cd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current configuration for feature engineering is: {'impute': True, 'one_hot_encoding': True, 'smote': True, 'smote_oversample_ratio': 0.8, 'smote_undersample_ratio': 1.0, 'robust_scaler': True, 'binning': True, 'outlier_handling': False, 'mutual_information': False, 'chi2_test': False}\n"
     ]
    }
   ],
   "source": [
    "with open(dir_path+\"/feature_engineering/feature_flag.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)[\"feature_engineering\"]\n",
    "\n",
    "print(f\"Current configuration for feature engineering is: {config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab0c6fb-1024-48f5-98ef-fb3c54cd12bc",
   "metadata": {},
   "source": [
    "## 1.1 Import libraries and reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71999b8a-6225-4e5e-baf9-b9508e253943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from  preprocessing import *\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "import warnings as wr\n",
    "wr.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3861ac05-ff21-424a-a351-9285eeb42783",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = pd.read_csv(f\"{data_folder}/Base_backup.csv\", header=0)\n",
    "df = df_base.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b8e8cd-cb1c-4584-8f6a-c919f537c737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from pylab import rcParams\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "my_palette = sns.color_palette(\"Paired\")\n",
    "\n",
    "from scipy.stats import chi2_contingency, pearsonr\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import chi2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8b8378-d7fd-418c-b599-ab0fe60f7530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split categorical and numerical features\n",
    "def split_num_cat(df):\n",
    "    \"\"\"\n",
    "    Function to split columns into two, one having categorical features and another having numerical feautures\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "            pass in full dataframe\n",
    "    ----------\n",
    "    Returns: \n",
    "        a list of categorical features\n",
    "        a list of numerical features\n",
    "    \"\"\"\n",
    "    categorical_features = []\n",
    "    numerical_features = []\n",
    "\n",
    "    for x in df.columns: \n",
    "        if df[x].nunique() > 12:\n",
    "            numerical_features.append(x)\n",
    "        elif df[x].nunique() >=2:\n",
    "            categorical_features.append(x)\n",
    "\n",
    "    if 'fraud_bool' in categorical_features:\n",
    "        categorical_features.remove('fraud_bool')\n",
    "    \n",
    "    return categorical_features, numerical_features\n",
    "\n",
    "def drop_columns(df, column_name):\n",
    "    \"\"\"\n",
    "    Function to delete the list of columns \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "            pass in full dataframe\n",
    "    column_name : list\n",
    "            pass in list of full column\n",
    "    ----------\n",
    "    Returns: Dataframe\n",
    "    \"\"\"\n",
    "    df = df.drop(column_name, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fedc08-12c2-417e-bf08-2d362c90b01f",
   "metadata": {},
   "source": [
    "## 1.2 Drop features with no variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58137586-3887-4a74-961c-40cad6d9183c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping the constant features: ['device_fraud_count']\n"
     ]
    }
   ],
   "source": [
    "constant_feature =[]\n",
    "for x in df.columns:\n",
    "    if df[x].nunique() == 1:\n",
    "        constant_feature.append(x)\n",
    "print(\"Dropping the constant features:\", constant_feature)       \n",
    "df = drop_columns(df, df[constant_feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cb48e6-24f3-48dd-b29a-f179a6943080",
   "metadata": {},
   "source": [
    "### 1.3 Change the dataype of binary features into type boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e25dfa0-06a7-40fc-a394-8cbbe8acba0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing the dataype of binary features into type boolean\n",
      "fraud_bool            bool\n",
      "email_is_free         bool\n",
      "phone_home_valid      bool\n",
      "phone_mobile_valid    bool\n",
      "has_other_cards       bool\n",
      "foreign_request       bool\n",
      "keep_alive_session    bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Changing the dataype of binary features into type boolean\")\n",
    "binary_features = df.columns[df.nunique() == 2].tolist()\n",
    "\n",
    "binary_features.remove('source')\n",
    "# Convert these features\"to boolean\n",
    "df[binary_features] = df[binary_features].astype(bool)\n",
    "\n",
    "# Verify changes\n",
    "print(df[binary_features].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbad7ac-6f1e-4359-9e82-fc7d882c95e3",
   "metadata": {},
   "source": [
    "## 1.3 Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d475a78-740e-4faa-ae2f-37ae8cdc7b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting train/test sets\n",
      "Training set (X_train) before feature engineering: (800000, 30)\n",
      "Test set (X_test) before feature engineering: (200000, 30)\n",
      "Training set (y_train) before feature engineering: (800000,)\n",
      "Test set (y_test) before feature engineering: (200000,)\n",
      "Categorical features before feature engineering: ['income', 'customer_age', 'payment_type', 'employment_status', 'email_is_free', 'housing_status', 'phone_home_valid', 'phone_mobile_valid', 'has_other_cards', 'proposed_credit_limit', 'foreign_request', 'source', 'device_os', 'keep_alive_session', 'device_distinct_emails_8w', 'month']\n",
      "Numerical features before feature engineering: ['name_email_similarity', 'prev_address_months_count', 'current_address_months_count', 'days_since_request', 'intended_balcon_amount', 'zip_count_4w', 'velocity_6h', 'velocity_24h', 'velocity_4w', 'bank_branch_count_8w', 'date_of_birth_distinct_emails_4w', 'credit_risk_score', 'bank_months_count', 'session_length_in_minutes']\n"
     ]
    }
   ],
   "source": [
    "print(\"splitting train/test sets\")\n",
    "y = df['fraud_bool']\n",
    "X = df.drop(columns=['fraud_bool'], axis = 1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "print(f\"Training set (X_train) before feature engineering: {X_train.shape}\")\n",
    "print(f\"Test set (X_test) before feature engineering: {X_test.shape}\")\n",
    "print(f\"Training set (y_train) before feature engineering: {y_train.shape}\")\n",
    "print(f\"Test set (y_test) before feature engineering: {y_test.shape}\")\n",
    "\n",
    "categorical_features, numerical_features = split_num_cat(df)\n",
    "print('Categorical features before feature engineering:', categorical_features)\n",
    "print('Numerical features before feature engineering:', numerical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170fe7a5-07a1-406c-97c5-4a7c335f1cae",
   "metadata": {},
   "source": [
    "## 2. Switcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4303d85d-3a85-419d-8216-c6e38d501f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"impute\"]:\n",
    "    print(\"Applying imputation for missing values...\")\n",
    "    X_train, X_test = impute_missing_values(X_train, X_test)\n",
    "else:\n",
    "    print(\"Not applying imputation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c95f01-cecf-4e11-97d5-e728f3c2c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"one_hot_encoding\"]:\n",
    "    print(\"Applying one-hot encoding...\")\n",
    "    X_train, X_test = one_hot_encode(X_train, X_test)\n",
    "else:\n",
    "    print(\"Not applying one hot encoder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5dd5d4-99bb-4487-b1af-680f23b7da95",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"binning\"]:\n",
    "    print(\"Applying binning...\")\n",
    "    X_train, X_test = bin_bank_months_count(X_train, X_test, y_train)\n",
    "else:\n",
    "    print(\"Not applying binning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02b603e-a4be-4b17-8197-b16dc1b47456",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if config[\"robust_scaler\"]:\n",
    "    print(\"Applying robust scaling...\")\n",
    "    X_train, X_test = robust_scaler(X_train, X_test)\n",
    "else:\n",
    "    print(\"Not applying scaler.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3dffff-558d-4630-8512-8fc26e21c9bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if config[\"outlier_handling\"]:\n",
    "    print(\"Removing outliers...\")\n",
    "    X_train, y_train = handle_outliers(X_train, y_train)\n",
    "else:\n",
    "    print(\"Not handling outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faf2942-8f4c-4a4a-bd51-d853b87fa8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"smote\"]:\n",
    "    print(\"Perfoming SMOTE to handle class imbalance\")\n",
    "    X_train, y_train = smote(X_train, y_train, over_ratio=0.7, under_ratio=0.9)\n",
    "else:\n",
    "    print(\"Not applying SMOTE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dc5787-b1d8-4fd2-ba8e-99b254de8934",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"mutual_information\"]:\n",
    "    print(\"Calculating mutual information scores based on the final training set\")\n",
    "    mi_scores = mutual_information(X_train, y_train)\n",
    "else:\n",
    "    print(\"Not calculating mutua information\")\n",
    "    \n",
    "if config[\"chi2_test\"]:\n",
    "    print(\"Calculating chi2 based on the final training set\")\n",
    "    chi2_results = chi2_test(X_train, y_train)\n",
    "else:\n",
    "    print(\"Not calculating chi2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab01dd72-2191-4c31-8bf1-7b0dc3219cc4",
   "metadata": {},
   "source": [
    "# Save the final output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4822860-dfcb-44d9-b4db-6fd338545cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_final_df(X_train, y_train, X_test, y_test):\n",
    "    bool_features = [col for col in X_train.columns if X_train[col].dtypes == 'bool']\n",
    "    X_train[bool_features] = X_train[bool_features].astype(\"int\")\n",
    "    y_train = y_train.astype(\"int\")\n",
    "    X_test[bool_features] = X_test[bool_features].astype(\"int\")\n",
    "    y_test = y_test.astype(\"int\")\n",
    "\n",
    "    print(f'Final X_train shape {X_train.shape}')\n",
    "    print(f'Final y_train shape {y_train.shape}')\n",
    "    print(f'Final X_test shape {X_test.shape}')\n",
    "    print(f'Final y_test shape {y_test.shape}')\n",
    "\n",
    "    print(\"Feature engineering is done. Exporting the final training data and test data to location:\", data_folder)\n",
    "    \n",
    "    X_train.to_csv(data_folder + \"/x_train_data.csv\", index=True)\n",
    "    X_test.to_csv(data_folder + \"/x_test_data.csv\")\n",
    "    \n",
    "    y_train.to_csv(data_folder + \"/y_train_data.csv\")\n",
    "    y_test.to_csv(data_folder + \"/y_test_data.csv\")\n",
    "    print(\"Data successfully exported into csv!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb25ba3f-9a15-4ffa-9b59-f1d81992a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_final_df(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c19d0b-b86f-4790-b50f-4bf4318f6c96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f082853a-f700-489d-8caf-16222f3016e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660b3b3e-6fe4-4d1e-9b56-0094e1375a01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "753b4513-99d8-4cea-a20e-6e397fba181d",
   "metadata": {},
   "source": [
    "# 2. Handle missing values¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8be95a-eb30-4f07-b932-c4159f9dbd6b",
   "metadata": {},
   "source": [
    "XGBoost supports missing values by default. In tree algorithms, branch directions for missing values are learned during training. And the gblinear booster treats missing values as zeros.\n",
    "\n",
    "We still choose to handle missing values in case of we are using any other types of models. \n",
    "Also for features with high percentage of missing values, we choose to add an additional column to indicate where the value is missing, so the model can still learn from missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c0e308-35a3-426a-89f1-bae97ccfafea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Imputes missing values in X_train and X_test based on predefined rules.\n",
    "\n",
    "    - Replaces `-1` values with `NaN`.\n",
    "    - Median imputation for specified columns.\n",
    "    - Bank months count missing values replaced with `0` and tracked in a new indicator column.\n",
    "    - Previous address months count imputed based on address bucket medians.\n",
    "\n",
    "    Parameters:\n",
    "    X_train (pd.DataFrame): Training dataset\n",
    "    X_test (pd.DataFrame): Test dataset\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame, pd.DataFrame: Imputed training and test datasets\n",
    "    \"\"\"\n",
    "\n",
    "    # Identify columns with missing values represented by -1\n",
    "    missing_value_cols = [col for col in X_train.columns if (X_train[col].min() == -1)]\n",
    "    print(\"Features with missing values represented by -1:\")\n",
    "    print(missing_value_cols)\n",
    "\n",
    "    # Replace -1 values with NaN\n",
    "    X_train[missing_value_cols] = X_train[missing_value_cols].replace(-1, np.nan)\n",
    "    X_test[missing_value_cols] = X_test[missing_value_cols].replace(-1, np.nan)\n",
    "\n",
    "    # Create copies to avoid modifying the original data\n",
    "    df_train_imputed = X_train.copy()\n",
    "    df_test_imputed = X_test.copy()\n",
    "\n",
    "    # Step 1: Median Imputation for selected columns\n",
    "    columns_to_impute = ['current_address_months_count', 'session_length_in_minutes', 'device_distinct_emails_8w']\n",
    "\n",
    "    medians = df_train_imputed[columns_to_impute].median()\n",
    "\n",
    "    for column in columns_to_impute:\n",
    "        df_train_imputed[column] = df_train_imputed[column].fillna(medians[column])\n",
    "        df_test_imputed[column] = df_test_imputed[column].fillna(medians[column])\n",
    "\n",
    "    # Step 2: Bank months count - fill missing with 0 and track missing values\n",
    "    df_train_imputed['bank_months_count_was_missing'] = df_train_imputed['bank_months_count'].isna().astype(int)\n",
    "    df_test_imputed['bank_months_count_was_missing'] = df_test_imputed['bank_months_count'].isna().astype(int)\n",
    "\n",
    "    df_train_imputed['bank_months_count'] = df_train_imputed['bank_months_count'].fillna(0)\n",
    "    df_test_imputed['bank_months_count'] = df_test_imputed['bank_months_count'].fillna(0)\n",
    "\n",
    "    # Step 3: Bucket-Based Imputation for prev_address_months_count\n",
    "    bin_edges = pd.cut(df_train_imputed['current_address_months_count'], 12, retbins=True)[1]\n",
    "\n",
    "    df_train_imputed['current_address_bucket'] = pd.cut(df_train_imputed['current_address_months_count'], bins=bin_edges)\n",
    "    df_test_imputed['current_address_bucket'] = pd.cut(df_test_imputed['current_address_months_count'], bins=bin_edges)\n",
    "\n",
    "    # Compute medians for each bucket\n",
    "    bucket_medians = df_train_imputed.groupby('current_address_bucket')['prev_address_months_count'].median()\n",
    "\n",
    "    # Indicator column for missing values\n",
    "    df_train_imputed['prev_address_months_count_was_missing'] = df_train_imputed['prev_address_months_count'].isna().astype(int)\n",
    "    df_test_imputed['prev_address_months_count_was_missing'] = df_test_imputed['prev_address_months_count'].isna().astype(int)\n",
    "\n",
    "    # Train set: fill missing using bucket median\n",
    "    df_train_imputed['prev_address_months_count'] = df_train_imputed.groupby('current_address_bucket')['prev_address_months_count'].transform(\n",
    "        lambda x: x.fillna(x.median())\n",
    "    )\n",
    "\n",
    "    # Test set: apply training bucket medians\n",
    "    df_test_imputed['prev_address_months_count'] = df_test_imputed.apply(\n",
    "        lambda row: bucket_medians[row['current_address_bucket']]\n",
    "        if pd.isna(row['prev_address_months_count']) and row['current_address_bucket'] in bucket_medians\n",
    "        else row['prev_address_months_count'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Drop temporary bucket column\n",
    "    df_train_imputed.drop(columns=['current_address_bucket'], inplace=True)\n",
    "    df_test_imputed.drop(columns=['current_address_bucket'], inplace=True)\n",
    "\n",
    "    # Validate that all missing values are handled\n",
    "    print(\"Train set null values after imputation:\")\n",
    "    print(df_train_imputed[missing_value_cols].isna().sum())\n",
    "\n",
    "    print(\"\\nTest set null values after imputation:\")\n",
    "    print(df_test_imputed[missing_value_cols].isna().sum())\n",
    "    print(f\"Training set shape after imputation: {df_train_imputed.shape}\")\n",
    "    print(f\"Test set shape after imputation: {df_test_imputed.shape}\")\n",
    "\n",
    "    return df_train_imputed, df_test_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fef091-6330-44fb-a21c-abe6681c0e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcf5a72-03f3-4707-826c-457d0a1ac635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca802b63-6844-4470-a476-4ea9678f6b63",
   "metadata": {},
   "source": [
    "## 3. One-hot encode for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34a4cfd-9ea9-47cc-bf58-c9f17dac8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def one_hot_encode(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Applies one-hot encoding to categorical features in the dataset.\n",
    "\n",
    "    - Identifies categorical features (dtype = 'object')\n",
    "    - Uses OneHotEncoder to transform categorical features\n",
    "    - Merges encoded columns with numerical features\n",
    "    - Ensures consistent encoding across train and test sets\n",
    "\n",
    "    Parameters:\n",
    "    X_train (pd.DataFrame): Training dataset\n",
    "    X_test (pd.DataFrame): Test dataset\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame, pd.DataFrame: One-hot encoded training and test datasets\n",
    "    \"\"\"\n",
    "\n",
    "    # Identify categorical features\n",
    "    categorical_features = [col for col in X_train.columns if X_train[col].dtypes == 'object']\n",
    "    print(\"Categorical features to encode:\", categorical_features)\n",
    "\n",
    "    if not categorical_features:\n",
    "        print(\"No categorical features found. Skipping one-hot encoding.\")\n",
    "        return X_train, X_test\n",
    "\n",
    "    # Initialize one-hot encoder\n",
    "    ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "    # Encode training and test sets\n",
    "    ohe_X_train = pd.DataFrame(ohe.fit_transform(X_train[categorical_features]))\n",
    "    ohe_X_test = pd.DataFrame(ohe.transform(X_test[categorical_features]))\n",
    "\n",
    "    # Set index to match original data\n",
    "    ohe_X_train.index = X_train.index\n",
    "    ohe_X_test.index = X_test.index\n",
    "\n",
    "    # Set column names to match encoded feature names\n",
    "    ohe_feature_names = ohe.get_feature_names_out(input_features=categorical_features)\n",
    "    ohe_X_train.columns = ohe_feature_names\n",
    "    ohe_X_test.columns = ohe_feature_names\n",
    "\n",
    "    # Drop categorical features and concatenate with encoded data\n",
    "    num_X_train = X_train.drop(categorical_features, axis=1)\n",
    "    num_X_test = X_test.drop(categorical_features, axis=1)\n",
    "\n",
    "    X_train_encoded = pd.concat([num_X_train, ohe_X_train], axis=1)\n",
    "    X_test_encoded = pd.concat([num_X_test, ohe_X_test], axis=1)\n",
    "\n",
    "    print(f\"One-hot encoded training shape: {X_train_encoded.shape}\")\n",
    "    print(f\"One-hot encoded test shape: {X_test_encoded.shape}\")\n",
    "\n",
    "    return X_train_encoded, X_test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138c2a86-88b5-476f-a310-afc882318513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e8f828-57b8-4d7c-b3bb-39d939c5c401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f055bb71-8114-41a5-88b6-2d4f4c46a483",
   "metadata": {},
   "source": [
    "# 4. Binning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed1f13d-2b48-4ab0-85ea-04c16d301707",
   "metadata": {},
   "source": [
    "Binning the bank_months_count, to turn it into a categorical variable for lower cardinality. \n",
    "\n",
    "Don't want to bin any other features as we don't want to lose details of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bc945f-d651-46d7-85c2-2cf37f01149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def bin_bank_months_count(X_train, X_test, y_train):\n",
    "    \"\"\"\n",
    "    Bins the 'bank_months_count' feature into custom intervals and replaces them with pre-defined medians.\n",
    "\n",
    "    - First bin includes only 0, the rest are grouped in intervals of 4\n",
    "    - Applies binning to both training and test datasets\n",
    "    - Plots fraud proportion against binned values\n",
    "    - Drops the original 'bank_months_count' column after transformation\n",
    "\n",
    "    Parameters:\n",
    "    X_train (pd.DataFrame): Training dataset\n",
    "    X_test (pd.DataFrame): Test dataset\n",
    "    y_train (pd.Series): Target variable for training set (needed for visualization)\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame, pd.DataFrame: Training and test datasets with binned 'bank_months_count'\n",
    "    \"\"\"\n",
    "\n",
    "    # Make copies to avoid modifying original datasets\n",
    "    X_train_binned = X_train.copy()\n",
    "    X_test_binned = X_test.copy()\n",
    "\n",
    "    min_val = X_train_binned[\"bank_months_count\"].min()\n",
    "    print(f\"the minimum value of bank_months_count is {min_val}\")\n",
    "    # Define bins: First bin starts from min_val, and remaining are in intervals of 4\n",
    "    bins = [min_val, min_val + 1] + [i for i in range(int(min_val) + 5, 37, 4)]\n",
    "    \n",
    "    median_labels = [0, 2.5, 6.5, 10.5, 14.5, 18.5, 22.5, 26.5, 30.5]  # Median of each bin\n",
    "\n",
    "    print(\"\\n Bin Ranges:\")\n",
    "    for i in range(len(bins) - 1):\n",
    "        print(f\"Bin {i+1}: [{bins[i]}, {bins[i+1]}) -> Median: {median_labels[i]}\")\n",
    "\n",
    "    # Apply binning transformation\n",
    "    X_train_binned[\"bank_months_count_binned\"] = pd.cut(\n",
    "        X_train_binned[\"bank_months_count\"], bins=bins, labels=median_labels, include_lowest=True, right=False\n",
    "    ).astype(float)\n",
    "\n",
    "    X_test_binned[\"bank_months_count_binned\"] = pd.cut(\n",
    "        X_test_binned[\"bank_months_count\"], bins=bins, labels=median_labels, include_lowest=True, right=False\n",
    "    ).astype(float)\n",
    "\n",
    "    # Display bin distribution\n",
    "    bin_counts = X_train_binned[\"bank_months_count_binned\"].value_counts().sort_index()\n",
    "    print(\"\\n Bin medians and counts in training set:\\n\")\n",
    "    print(bin_counts)\n",
    "\n",
    "    # Visualization: Fraud proportion per bin\n",
    "    X_train_binned[\"fraud_bool\"] = y_train  # Temporarily add fraud labels for visualization\n",
    "    fraud_proportion = X_train_binned.groupby(\"bank_months_count_binned\")[\"fraud_bool\"].mean()\n",
    "\n",
    "    plot_data = (\n",
    "        X_train_binned[X_train_binned[\"fraud_bool\"] == 1][\"bank_months_count_binned\"]\n",
    "        .value_counts(normalize=True)\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    # Plot fraud proportion per bin\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plot_data.plot(kind=\"bar\", color=\"#fc8d62\", edgecolor=\"black\")\n",
    "    plt.xlabel(\"Bank Months Count (Binned)\")\n",
    "    plt.ylabel(\"Proportion of Fraud (fraud_bool = 1)\")\n",
    "    plt.title(\"Proportion of Fraud by Bank Months Count (Binned)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Remove temporary fraud column\n",
    "    X_train_binned.drop(columns=[\"fraud_bool\"], inplace=True)\n",
    "\n",
    "    # Drop original 'bank_months_count' after binning\n",
    "    X_train_binned.drop(columns=\"bank_months_count\", inplace=True)\n",
    "    X_test_binned.drop(columns=\"bank_months_count\", inplace=True)\n",
    "\n",
    "    print(f\"Final shape after binning - Train: {X_train_binned.shape}, Test: {X_test_binned.shape}\")\n",
    "\n",
    "    return X_train_binned, X_test_binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413abfd3-c018-4bd9-80be-00dab79bf7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0e3d12e-c35e-436c-87c1-2dd568c49b04",
   "metadata": {},
   "source": [
    "# 5. Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dec1a7-55f5-4662-8c70-24e9a5823a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "def robust_scaler(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Apply robust scaler on numerical features. Robust scaler scale the data based on IQR and median, \n",
    "    which maeks the dataset more robust against outliers. https://proclusacademy.com/blog/robust-scaler-outliers/ \n",
    "\n",
    "    - First split the dataset into numerical and categorical features. \n",
    "    - Then import RobustScaler from sklearn\n",
    "    - Fit robust scaler on the training set, then transform numerical_features in both training and test set. \n",
    "    - Add a 'scaled_' prefix to the columns that are scaled, then drop the original column.\n",
    "\n",
    "    Parameters:\n",
    "    X_train (pd.DataFrame): Training dataset\n",
    "    X_test (pd.DataFrame): Test dataset\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame, pd.DataFrame: Training and test datasets with scaled numerical features\n",
    "    \"\"\"\n",
    "    categorical_features, numerical_features = split_num_cat(X_train)\n",
    "    print('Categorical features:', categorical_features)\n",
    "    print('Numerical features:', numerical_features)\n",
    "    \n",
    "    X_train[numerical_features].describe()\n",
    "    \n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "     \n",
    "    robust_scaler = RobustScaler()\n",
    "    \n",
    "    scaled_train = robust_scaler.fit_transform(X_train_scaled[numerical_features])\n",
    "    scaled_test = robust_scaler.transform(X_test_scaled[numerical_features])\n",
    "    \n",
    "    # add new columns scaled features while keeping the original feature with the unscaled values. \n",
    "    for i, feature in enumerate(numerical_features):\n",
    "        X_train_scaled['scaled_' + feature] = scaled_train[:, i]\n",
    "        X_test_scaled['scaled_' + feature] = scaled_test[:, i]\n",
    "    \n",
    "    print(X_train_scaled.describe())\n",
    "    \n",
    "    # drop the original columns before scaling:\n",
    "    X_train_scaled.drop(columns=numerical_features, inplace=True)\n",
    "    X_test_scaled.drop(columns=numerical_features, inplace=True)\n",
    "\n",
    "    print(f\"Final shape after binning - Train: {X_train_scaled.shape}, Test: {X_test_scaled.shape}\")\n",
    "\n",
    "    return X_train_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6786d6-f52d-4492-a683-81372fe04924",
   "metadata": {},
   "source": [
    "help with unsupervised learning, minimise bias against one variabel "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ce2dd7-04e5-4a6b-a4fa-58060be99915",
   "metadata": {},
   "source": [
    "# 5. Handle outliers - not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f10cd6-77f3-410e-8c95-c6ee83e62df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(series):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return (series < lower_bound) | (series > upper_bound)\n",
    "\n",
    "def handle_outliers(X_train, y_train):\n",
    "    categorical_features, numerical_features = split_num_cat(X_train)\n",
    "    print('Categorical features:', categorical_features)\n",
    "    print('Numerical features:', numerical_features)\n",
    "\n",
    "    # Detect outliers in numerical features\n",
    "    total_outliers = X_train[numerical_features].apply(detect_outliers).sum()\n",
    "    outlier_percentage = (total_outliers / len(X_train)) * 100\n",
    "    print(\"Percentage of outliers per numerical variable\\n\", outlier_percentage)\n",
    "\n",
    "    # For all fraud cases, calculate the percentage of outliers\n",
    "    X_train_fraud_only = X_train[y_train == 1].copy()\n",
    "    total_outlier_fraud_only = detect_outliers(X_train_fraud_only[numerical_features]).sum()\n",
    "    outlier_percentages_fraud_only = (total_outlier_fraud_only / len(X_train_fraud_only)) * 100\n",
    "    print(\"Percentage of outliers among the fraud cases (fraud_bool = 1):\")\n",
    "    print(outlier_percentages_fraud_only)\n",
    "\n",
    "    # Calculate fraud outlier percentages per feature\n",
    "    outlier_fraud_percentages = {}\n",
    "    for feature in numerical_features:\n",
    "        outliers = detect_outliers(X_train[feature])\n",
    "        fraud_outliers = X_train.loc[outliers & (y_train == 1), feature]\n",
    "        percentage_fraud_outliers = (len(fraud_outliers) / len(X_train[X_train[feature].notna()])) * 100\n",
    "        outlier_fraud_percentages[feature] = percentage_fraud_outliers\n",
    "    print(\"Percentage of outliers that are fraud:\")\n",
    "    print(outlier_fraud_percentages)\n",
    "\n",
    "    # Remove outliers using the 1st and 99th percentile\n",
    "    q1 = X_train[numerical_features].quantile(0.01)\n",
    "    q99 = X_train[numerical_features].quantile(0.99)\n",
    "    X_train_cleaned = X_train[(X_train[numerical_features] >= q1).all(axis=1) &\n",
    "                              (X_train[numerical_features] <= q99).all(axis=1)]\n",
    "    y_train_cleaned = y_train.loc[X_train_cleaned.index].copy()\n",
    "\n",
    "    # Print percentage of retained data\n",
    "    percentage_retained = (len(X_train_cleaned) / len(X_train)) * 100\n",
    "    print(f\"Percentage of data retained after dropping outliers: {percentage_retained:.2f}%\")\n",
    "\n",
    "    # Calculate percentage of fraud cases retained\n",
    "    fraud_indices = y_train[y_train == 1].index\n",
    "    fraud_retained = len(fraud_indices.intersection(X_train_cleaned.index))\n",
    "    percentage_fraud_retained = (fraud_retained / len(fraud_indices)) * 100\n",
    "    print(f\"Percentage of fraud_bool = 1 retained in cleaned data: {percentage_fraud_retained:.2f}%\")\n",
    "    \n",
    "    print(f\"Final X_train shape: {X_train_cleaned.shape}\")\n",
    "    print(f\"Final y_train shape: {y_train_cleaned.shape}\")\n",
    "\n",
    "    return X_train_cleaned, y_train_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b3567-c090-406f-af2c-8527de389084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76f11c6e-2cbe-4cbe-87fa-b4bd751f0033",
   "metadata": {},
   "source": [
    "# Handle imbalance with SMOTE, only on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af8126a-8dac-4914-bf62-acc74a2f5130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "def smote(X_train, y_train, over_ratio=0.8, under_ratio=1.0):\n",
    "    \"\"\"\n",
    "    Applies SMOTE for oversampling and RandomUnderSampler for undersampling.\n",
    "    \n",
    "    :param X_train: Feature matrix.\n",
    "    :param y_train: Target vector.\n",
    "    :param over_ratio: Oversampling ratio for the minority class (default: 0.8).\n",
    "    :param under_ratio: Undersampling ratio for the majority class after oversampling (default: 1.0).\n",
    "    :return: Resampled X_train and y_train.\n",
    "    \"\"\"\n",
    "    print(f\"Before SMOTE: {Counter(y_train)}\")\n",
    "    print(f\"Before SMOTE, shape of the training set: {X_train.shape}\")\n",
    "\n",
    "    # Oversample the minority class\n",
    "    over = SMOTE(sampling_strategy=over_ratio, random_state=2)\n",
    "    X_train_res, y_train_res = over.fit_resample(X_train, y_train)\n",
    "\n",
    "    print(f\"After oversampling: {Counter(y_train_res)}\")\n",
    "\n",
    "    # Undersample the majority class\n",
    "    under = RandomUnderSampler(sampling_strategy=under_ratio, random_state=2)\n",
    "    X_train_resampled, y_train_resampled = under.fit_resample(X_train_res, y_train_res)\n",
    "\n",
    "    print(f\"After undersampling: {Counter(y_train_resampled)}\")\n",
    "    print(f\"After undersampling, shape of the training set: {X_train_resampled.shape}\")\n",
    "\n",
    "    return X_train_resampled, y_train_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912cffa7-1aa6-4efc-b445-453aceff09f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "over = SMOTE(sampling_strategy=1, random_state=2)\n",
    "X_train_res5, y_train_res5 = over.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"After SMOTE: {Counter(y_train_res5)}\") \n",
    "\n",
    "# Undersample the majority class to create a 1:1 minority-to-majority ratio\n",
    "under = RandomUnderSampler(sampling_strategy=1, random_state=2)\n",
    "X_train_res6, y_train_res6 = under.fit_resample(X_train_res5, y_train_res5)\n",
    "\n",
    "print(f\"After RandomUnderSampler: {Counter(y_train_res6)}\")\n",
    "print(\"After RandomUnderSampler, shape of the training set\", y_train_res6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498ea451-787a-44ef-a843-cde50198e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_res6.to_csv(\"../data/x_train_data_smote3.csv\", index=True)\n",
    "\n",
    "y_train_res6.to_csv(\"../data/y_train_data_smote3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecc3e45-45a4-4077-bfce-507757bd83c1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05d4900-3676-413c-ae46-3cb95de0bd72",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1565950-b3e8-4b57-b6c4-526cf97e5a8c",
   "metadata": {},
   "source": [
    "## Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdd1848-e3b7-4f94-8c62-efa4a38d86bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import mutual_info_regression, chi2, SelectKBest\n",
    "\n",
    "def mutual_information(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Compute Mutual Information scores for all features.\n",
    "    \"\"\"\n",
    "    X = X_train.copy()\n",
    "    y = y_train.copy()\n",
    "\n",
    "    # Label encode categorical features\n",
    "    for colname in X.select_dtypes([\"category\", \"object\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "\n",
    "    # Determine discrete features\n",
    "    discrete_features = X.dtypes == int\n",
    "\n",
    "    # Compute Mutual Information scores\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "    print(\"Mutual Information Scores:\")\n",
    "    print(mi_scores)\n",
    "\n",
    "    def plot_mi_scores(scores):\n",
    "        scores = scores.sort_values(ascending=True)\n",
    "        width = np.arange(len(scores))\n",
    "        ticks = list(scores.index)\n",
    "        plt.barh(width, scores)\n",
    "        plt.yticks(width, ticks)\n",
    "        plt.title(\"Mutual Information Scores\")\n",
    "        plt.show()\n",
    "\n",
    "    plt.figure(dpi=100, figsize=(8, 16))\n",
    "    plot_mi_scores(mi_scores)\n",
    "\n",
    "    return mi_scores\n",
    "\n",
    "\n",
    "def chi2_test(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Perform Chi-Square test for categorical features.\n",
    "    \"\"\"\n",
    "    categorical_features, numerical_features = split_num_cat(X_train)\n",
    "    X_cat = X_train[categorical_features].copy()\n",
    "    chi2_test = SelectKBest(score_func=chi2).fit(X_cat, y_train)\n",
    "\n",
    "    chi2_output = pd.DataFrame({\n",
    "        'feature': X_cat.columns,\n",
    "        'chi2_score': chi2_test.scores_,\n",
    "        'p_value': chi2_test.pvalues_\n",
    "    }).sort_values(by=['p_value'])\n",
    "\n",
    "    print(\"Chi-Square Test Results:\")\n",
    "    print(chi2_output)\n",
    "\n",
    "    # Filter significant features\n",
    "    chi2_output_significant = chi2_output[chi2_output['p_value'] <= 0.05]\n",
    "\n",
    "    # Plot significant chi2 scores\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.barplot(data=chi2_output_significant, x='chi2_score', y='feature')\n",
    "    plt.title(\"Significant Chi-Square Scores\")\n",
    "    plt.show()\n",
    "\n",
    "    return chi2_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28dd3aa-abdd-421c-ae17-a15fcede864a",
   "metadata": {},
   "source": [
    "MI can help you to understand the relative potential of a feature as a predictor of the target, considered by itself.\n",
    "It's possible for a feature to be very informative when interacting with other features, but not so informative all alone. MI can't detect interactions between features. It is a univariate metric.\n",
    "The actual usefulness of a feature depends on the model you use it with. A feature is only useful to the extent that its relationship with the target is one your model can learn. Just because a feature has a high MI score doesn't mean your model will be able to do anything with that information. You may need to transform the feature first to expose the association."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee56d1c-82f9-4d1b-88ea-c9311b152907",
   "metadata": {},
   "source": [
    "# Finalize data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6827b269-d090-4a39-9fcd-ebd48cef1dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_final_df(X_train, y_train, X_test, y_test):\n",
    "    bool_features = [col for col in X_train.columns if X_train[col].dtypes == 'bool']\n",
    "    X_train[bool_features] = X_train[bool_features].astype(\"int\")\n",
    "    y_train = y_train.astype(\"int\")\n",
    "    X_test[bool_features] = X_test[bool_features].astype(\"int\")\n",
    "    y_test = y_test.astype(\"int\")\n",
    "\n",
    "    print(f'Final X_train shape {X_train.shape}')\n",
    "    print(f'Final y_train shape {y_train.shape}')\n",
    "    print(f'Final X_test shape {X_test.shape}')\n",
    "    print(f'Final y_test shape {y_test.shape}')\n",
    "\n",
    "    print(\"Feature engineering is done. Exporting the final training data and test data to location:\", data_folder)\n",
    "    \n",
    "    X_train.to_csv(data_folder + \"/x_train_data.csv\", index=True)\n",
    "    X_test.to_csv(data_folder + \"/x_test_data.csv\")\n",
    "    \n",
    "    y_train.to_csv(data_folder + \"/y_train_data.csv\")\n",
    "    y_test.to_csv(data_folder + \"/y_test_data.csv\")\n",
    "    print(\"Data successfully exported into csv!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4ea01f-e5d4-453f-8926-fb9c1e43bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fb835d-1e38-41ed-93ac-974079b0533c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb9ae87-004f-4bee-94c2-e49e2bd3e6da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dedc3c6-e333-48b7-a31d-f63ee4583d31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fe93a4-dc7c-4cf6-aa3f-80c3a8abf521",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = X_train.copy()\n",
    "y_train_final = y_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3357d9d7-9e29-4778-8125-8cd287dbcb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_res4.copy()\n",
    "y_train = y_train_res4.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacdc134-9022-40e8-a5c9-6ca5c089597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_features = [col for col in X_train.columns if X_train[col].dtypes == 'bool']\n",
    "X_train[bool_features] = X_train[bool_features].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e8db46-1adc-4dcb-ba48-f460d7268b22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train[bool_features] = X_train[bool_features].astype(\"int\")\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2067b951-f8aa-4f70-8adb-6cdbe11e8f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(\"int\")\n",
    "y_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f8dd8f-09da-4368-ab3d-7a71e98d5fa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test[bool_features] = X_test[bool_features].astype(\"int\")\n",
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f4695a-210f-4ffc-a2f6-42857e1f1630",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test = y_test.astype(\"int\")\n",
    "y_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a01c62-56e3-4f7a-a9f7-03a72431a3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc1ac00-9c1a-4e86-a278-1122e27708a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5192bb40-b9d4-4a9f-8c6b-41a97d2fc820",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a89c65-9da4-4e79-81a2-e5e4fbd9f2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b39bf3-6d83-4836-bd90-99add573172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"../data/x_train_data.csv\", index=True)\n",
    "X_test.to_csv(\"../data/x_test_data.csv\")\n",
    "\n",
    "y_train.to_csv(\"../data/y_train_data.csv\")\n",
    "y_test.to_csv(\"../data/y_test_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
